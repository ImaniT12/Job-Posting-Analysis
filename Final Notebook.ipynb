{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fraudulent Job Detection\n",
    "#### By Imani Thompson"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This project analyses which job opening features Indeed should consider when determining whether or not they have a fraudulent job listed. Descriptive analysis i did on job posting predictions shows that job descriptions and job requirements together have a higher potential of identifying fraudulent activity. This was determine by the final model which had a 97% accuracy score. Natural language processing techniques were used on this binary classification model. Indeed can use these findings to get rid of fraudulent jobs on their website or give vistor direct access to my website created to predict the status. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Business Understanding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "My Stakeholder is Indeed, a worldwide employment website, that has thousands of job postings. An issue that they are encountering is the amount of fraudulent job postings. I was hired by this company to create a model to determine the amount of fraudulent job postings on their website. My predictions are important from a business prospective because I will be analysing actual data to get my answer and not just taking educated guesses. Even though I will not be modeling with Indeed data, it will still be useful to them because it uses the same concept. Determining whether or not a job opening is fruadulent is considered a binary classification, therefore it will require a binary classification model. Since job postings consist of a good bit of text I will be using natural language processing to model with text. Indeed would like a strong model that increases the amount of true negatives and have little to no false positives and false negative. A false positive in this context would be my model predicting that a job opening is fraudlent and it being incorrect. A true negative in this context would be my model predicting a job opening is not fraudulent and it being correct. A false negative in this context would be my model predicting a job opening is not fraudulent but it is. Both false positive and negative are bad but which is worst just depends on the situation. I am focused on optimising the accuracy score when modeling. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Understanding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data I used for my analyse is from kaggle. This dataset was made in 2020 and it consist of 18,000 rows which are individual job postings. This dataset has both textual information and meta-information about the jobs. Overall this dataset is related to the data analysis questions because it holds all the necessary information to determine what job postings are legit. The target varible within this dataset is the fraudlent column. Other variables included in this dataset are job job title, location, department, salary range, benefits, etc. The varibles that I will focus on are the descriptions and requirements. These two variables are objects and include missing values. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier, plot_tree\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, recall_score, precision_score, f1_score\n",
    "from sklearn.metrics import plot_confusion_matrix\n",
    "from sklearn.metrics import roc_auc_score, plot_roc_curve\n",
    "from collections import defaultdict\n",
    "import nltk\n",
    "from nltk.probability import FreqDist\n",
    "from nltk.corpus import stopwords, wordnet\n",
    "from nltk.tokenize import regexp_tokenize, word_tokenize, RegexpTokenizer\n",
    "from nltk import pos_tag\n",
    "import string\n",
    "import re\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer,\\\n",
    "HashingVectorizer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import accuracy_score, precision_score, confusion_matrix\n",
    "from sklearn.metrics import plot_confusion_matrix\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "from lime.lime_text import LimeTextExplainer\n",
    "from sklearn.pipeline import make_pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/imanithompson/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/imanithompson/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package tagsets to\n",
      "[nltk_data]     /Users/imanithompson/nltk_data...\n",
      "[nltk_data]   Package tagsets is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/imanithompson/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Use this to download the stopwords\n",
    "nltk.download(\"stopwords\")\n",
    "\n",
    "# Use this to download the wordnet\n",
    "nltk.download('wordnet')\n",
    "\n",
    "# Use this to download the parts of speech tagging\n",
    "nltk.download('tagsets')\n",
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading in the csv file\n",
    "data = pd.read_csv('../../../../Downloads/Data/fake_job_postings.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>job_id</th>\n",
       "      <th>title</th>\n",
       "      <th>location</th>\n",
       "      <th>department</th>\n",
       "      <th>salary_range</th>\n",
       "      <th>company_profile</th>\n",
       "      <th>description</th>\n",
       "      <th>requirements</th>\n",
       "      <th>benefits</th>\n",
       "      <th>telecommuting</th>\n",
       "      <th>has_company_logo</th>\n",
       "      <th>has_questions</th>\n",
       "      <th>employment_type</th>\n",
       "      <th>required_experience</th>\n",
       "      <th>required_education</th>\n",
       "      <th>industry</th>\n",
       "      <th>function</th>\n",
       "      <th>fraudulent</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Marketing Intern</td>\n",
       "      <td>US, NY, New York</td>\n",
       "      <td>Marketing</td>\n",
       "      <td>NaN</td>\n",
       "      <td>We're Food52, and we've created a groundbreaki...</td>\n",
       "      <td>Food52, a fast-growing, James Beard Award-winn...</td>\n",
       "      <td>Experience with content management systems a m...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>Other</td>\n",
       "      <td>Internship</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Marketing</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>Customer Service - Cloud Video Production</td>\n",
       "      <td>NZ, , Auckland</td>\n",
       "      <td>Success</td>\n",
       "      <td>NaN</td>\n",
       "      <td>90 Seconds, the worlds Cloud Video Production ...</td>\n",
       "      <td>Organised - Focused - Vibrant - Awesome!Do you...</td>\n",
       "      <td>What we expect from you:Your key responsibilit...</td>\n",
       "      <td>What you will get from usThrough being part of...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>Full-time</td>\n",
       "      <td>Not Applicable</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Marketing and Advertising</td>\n",
       "      <td>Customer Service</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>Commissioning Machinery Assistant (CMA)</td>\n",
       "      <td>US, IA, Wever</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Valor Services provides Workforce Solutions th...</td>\n",
       "      <td>Our client, located in Houston, is actively se...</td>\n",
       "      <td>Implement pre-commissioning and commissioning ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>Account Executive - Washington DC</td>\n",
       "      <td>US, DC, Washington</td>\n",
       "      <td>Sales</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Our passion for improving quality of life thro...</td>\n",
       "      <td>THE COMPANY: ESRI – Environmental Systems Rese...</td>\n",
       "      <td>EDUCATION: Bachelor’s or Master’s in GIS, busi...</td>\n",
       "      <td>Our culture is anything but corporate—we have ...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>Full-time</td>\n",
       "      <td>Mid-Senior level</td>\n",
       "      <td>Bachelor's Degree</td>\n",
       "      <td>Computer Software</td>\n",
       "      <td>Sales</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>Bill Review Manager</td>\n",
       "      <td>US, FL, Fort Worth</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>SpotSource Solutions LLC is a Global Human Cap...</td>\n",
       "      <td>JOB TITLE: Itemization Review ManagerLOCATION:...</td>\n",
       "      <td>QUALIFICATIONS:RN license in the State of Texa...</td>\n",
       "      <td>Full Benefits Offered</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Full-time</td>\n",
       "      <td>Mid-Senior level</td>\n",
       "      <td>Bachelor's Degree</td>\n",
       "      <td>Hospital &amp; Health Care</td>\n",
       "      <td>Health Care Provider</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   job_id                                      title            location  \\\n",
       "0       1                           Marketing Intern    US, NY, New York   \n",
       "1       2  Customer Service - Cloud Video Production      NZ, , Auckland   \n",
       "2       3    Commissioning Machinery Assistant (CMA)       US, IA, Wever   \n",
       "3       4          Account Executive - Washington DC  US, DC, Washington   \n",
       "4       5                        Bill Review Manager  US, FL, Fort Worth   \n",
       "\n",
       "  department salary_range                                    company_profile  \\\n",
       "0  Marketing          NaN  We're Food52, and we've created a groundbreaki...   \n",
       "1    Success          NaN  90 Seconds, the worlds Cloud Video Production ...   \n",
       "2        NaN          NaN  Valor Services provides Workforce Solutions th...   \n",
       "3      Sales          NaN  Our passion for improving quality of life thro...   \n",
       "4        NaN          NaN  SpotSource Solutions LLC is a Global Human Cap...   \n",
       "\n",
       "                                         description  \\\n",
       "0  Food52, a fast-growing, James Beard Award-winn...   \n",
       "1  Organised - Focused - Vibrant - Awesome!Do you...   \n",
       "2  Our client, located in Houston, is actively se...   \n",
       "3  THE COMPANY: ESRI – Environmental Systems Rese...   \n",
       "4  JOB TITLE: Itemization Review ManagerLOCATION:...   \n",
       "\n",
       "                                        requirements  \\\n",
       "0  Experience with content management systems a m...   \n",
       "1  What we expect from you:Your key responsibilit...   \n",
       "2  Implement pre-commissioning and commissioning ...   \n",
       "3  EDUCATION: Bachelor’s or Master’s in GIS, busi...   \n",
       "4  QUALIFICATIONS:RN license in the State of Texa...   \n",
       "\n",
       "                                            benefits  telecommuting  \\\n",
       "0                                                NaN              0   \n",
       "1  What you will get from usThrough being part of...              0   \n",
       "2                                                NaN              0   \n",
       "3  Our culture is anything but corporate—we have ...              0   \n",
       "4                              Full Benefits Offered              0   \n",
       "\n",
       "   has_company_logo  has_questions employment_type required_experience  \\\n",
       "0                 1              0           Other          Internship   \n",
       "1                 1              0       Full-time      Not Applicable   \n",
       "2                 1              0             NaN                 NaN   \n",
       "3                 1              0       Full-time    Mid-Senior level   \n",
       "4                 1              1       Full-time    Mid-Senior level   \n",
       "\n",
       "  required_education                   industry              function  \\\n",
       "0                NaN                        NaN             Marketing   \n",
       "1                NaN  Marketing and Advertising      Customer Service   \n",
       "2                NaN                        NaN                   NaN   \n",
       "3  Bachelor's Degree          Computer Software                 Sales   \n",
       "4  Bachelor's Degree     Hospital & Health Care  Health Care Provider   \n",
       "\n",
       "   fraudulent  \n",
       "0           0  \n",
       "1           0  \n",
       "2           0  \n",
       "3           0  \n",
       "4           0  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Showing the first 5 rows of the dataframe\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['job_id', 'title', 'location', 'department', 'salary_range',\n",
       "       'company_profile', 'description', 'requirements', 'benefits',\n",
       "       'telecommuting', 'has_company_logo', 'has_questions', 'employment_type',\n",
       "       'required_experience', 'required_education', 'industry', 'function',\n",
       "       'fraudulent'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Showing the columns within the dataset\n",
    "data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>job_id</th>\n",
       "      <th>telecommuting</th>\n",
       "      <th>has_company_logo</th>\n",
       "      <th>has_questions</th>\n",
       "      <th>fraudulent</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>17880.000000</td>\n",
       "      <td>17880.000000</td>\n",
       "      <td>17880.000000</td>\n",
       "      <td>17880.000000</td>\n",
       "      <td>17880.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>8940.500000</td>\n",
       "      <td>0.042897</td>\n",
       "      <td>0.795302</td>\n",
       "      <td>0.491723</td>\n",
       "      <td>0.048434</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>5161.655742</td>\n",
       "      <td>0.202631</td>\n",
       "      <td>0.403492</td>\n",
       "      <td>0.499945</td>\n",
       "      <td>0.214688</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>4470.750000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>8940.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>13410.250000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>17880.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             job_id  telecommuting  has_company_logo  has_questions  \\\n",
       "count  17880.000000   17880.000000      17880.000000   17880.000000   \n",
       "mean    8940.500000       0.042897          0.795302       0.491723   \n",
       "std     5161.655742       0.202631          0.403492       0.499945   \n",
       "min        1.000000       0.000000          0.000000       0.000000   \n",
       "25%     4470.750000       0.000000          1.000000       0.000000   \n",
       "50%     8940.500000       0.000000          1.000000       0.000000   \n",
       "75%    13410.250000       0.000000          1.000000       1.000000   \n",
       "max    17880.000000       1.000000          1.000000       1.000000   \n",
       "\n",
       "         fraudulent  \n",
       "count  17880.000000  \n",
       "mean       0.048434  \n",
       "std        0.214688  \n",
       "min        0.000000  \n",
       "25%        0.000000  \n",
       "50%        0.000000  \n",
       "75%        0.000000  \n",
       "max        1.000000  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Description of numeric data in Dataframe\n",
    "data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>location</th>\n",
       "      <th>department</th>\n",
       "      <th>salary_range</th>\n",
       "      <th>company_profile</th>\n",
       "      <th>description</th>\n",
       "      <th>requirements</th>\n",
       "      <th>benefits</th>\n",
       "      <th>employment_type</th>\n",
       "      <th>required_experience</th>\n",
       "      <th>required_education</th>\n",
       "      <th>industry</th>\n",
       "      <th>function</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>17880</td>\n",
       "      <td>17534</td>\n",
       "      <td>6333</td>\n",
       "      <td>2868</td>\n",
       "      <td>14572</td>\n",
       "      <td>17879</td>\n",
       "      <td>15185</td>\n",
       "      <td>10670</td>\n",
       "      <td>14409</td>\n",
       "      <td>10830</td>\n",
       "      <td>9775</td>\n",
       "      <td>12977</td>\n",
       "      <td>11425</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>11231</td>\n",
       "      <td>3105</td>\n",
       "      <td>1337</td>\n",
       "      <td>874</td>\n",
       "      <td>1709</td>\n",
       "      <td>14801</td>\n",
       "      <td>11968</td>\n",
       "      <td>6205</td>\n",
       "      <td>5</td>\n",
       "      <td>7</td>\n",
       "      <td>13</td>\n",
       "      <td>131</td>\n",
       "      <td>37</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>English Teacher Abroad</td>\n",
       "      <td>GB, LND, London</td>\n",
       "      <td>Sales</td>\n",
       "      <td>0-0</td>\n",
       "      <td>We help teachers get safe &amp;amp; secure jobs ab...</td>\n",
       "      <td>Play with kids, get paid for it Love travel? J...</td>\n",
       "      <td>University degree required. TEFL / TESOL / CEL...</td>\n",
       "      <td>See job description</td>\n",
       "      <td>Full-time</td>\n",
       "      <td>Mid-Senior level</td>\n",
       "      <td>Bachelor's Degree</td>\n",
       "      <td>Information Technology and Services</td>\n",
       "      <td>Information Technology</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>311</td>\n",
       "      <td>718</td>\n",
       "      <td>551</td>\n",
       "      <td>142</td>\n",
       "      <td>726</td>\n",
       "      <td>379</td>\n",
       "      <td>410</td>\n",
       "      <td>726</td>\n",
       "      <td>11620</td>\n",
       "      <td>3809</td>\n",
       "      <td>5145</td>\n",
       "      <td>1734</td>\n",
       "      <td>1749</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          title         location department salary_range  \\\n",
       "count                     17880            17534       6333         2868   \n",
       "unique                    11231             3105       1337          874   \n",
       "top     English Teacher Abroad   GB, LND, London      Sales          0-0   \n",
       "freq                        311              718        551          142   \n",
       "\n",
       "                                          company_profile  \\\n",
       "count                                               14572   \n",
       "unique                                               1709   \n",
       "top     We help teachers get safe &amp; secure jobs ab...   \n",
       "freq                                                  726   \n",
       "\n",
       "                                              description  \\\n",
       "count                                               17879   \n",
       "unique                                              14801   \n",
       "top     Play with kids, get paid for it Love travel? J...   \n",
       "freq                                                  379   \n",
       "\n",
       "                                             requirements  \\\n",
       "count                                               15185   \n",
       "unique                                              11968   \n",
       "top     University degree required. TEFL / TESOL / CEL...   \n",
       "freq                                                  410   \n",
       "\n",
       "                   benefits employment_type required_experience  \\\n",
       "count                 10670           14409               10830   \n",
       "unique                 6205               5                   7   \n",
       "top     See job description       Full-time    Mid-Senior level   \n",
       "freq                    726           11620                3809   \n",
       "\n",
       "       required_education                             industry  \\\n",
       "count                9775                                12977   \n",
       "unique                 13                                  131   \n",
       "top     Bachelor's Degree  Information Technology and Services   \n",
       "freq                 5145                                 1734   \n",
       "\n",
       "                      function  \n",
       "count                    11425  \n",
       "unique                      37  \n",
       "top     Information Technology  \n",
       "freq                      1749  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Description of object data in Dataframe\n",
    "data.describe(include='O')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 17880 entries, 0 to 17879\n",
      "Data columns (total 18 columns):\n",
      " #   Column               Non-Null Count  Dtype \n",
      "---  ------               --------------  ----- \n",
      " 0   job_id               17880 non-null  int64 \n",
      " 1   title                17880 non-null  object\n",
      " 2   location             17534 non-null  object\n",
      " 3   department           6333 non-null   object\n",
      " 4   salary_range         2868 non-null   object\n",
      " 5   company_profile      14572 non-null  object\n",
      " 6   description          17879 non-null  object\n",
      " 7   requirements         15185 non-null  object\n",
      " 8   benefits             10670 non-null  object\n",
      " 9   telecommuting        17880 non-null  int64 \n",
      " 10  has_company_logo     17880 non-null  int64 \n",
      " 11  has_questions        17880 non-null  int64 \n",
      " 12  employment_type      14409 non-null  object\n",
      " 13  required_experience  10830 non-null  object\n",
      " 14  required_education   9775 non-null   object\n",
      " 15  industry             12977 non-null  object\n",
      " 16  function             11425 non-null  object\n",
      " 17  fraudulent           17880 non-null  int64 \n",
      "dtypes: int64(5), object(13)\n",
      "memory usage: 2.5+ MB\n"
     ]
    }
   ],
   "source": [
    "# Shows information about the Dataframe\n",
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(17880, 18)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Showing the shape of the dataframe\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since my main focus concerning this datatset is the job description and requiremnts, I am going to ignore all other columns. The columns `decription` and `requirements` have missing values so I dropped all of them. I also made a new column called `words` that is a combination of both description and requirements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 17880 entries, 0 to 17879\n",
      "Data columns (total 2 columns):\n",
      " #   Column        Non-Null Count  Dtype \n",
      "---  ------        --------------  ----- \n",
      " 0   description   17879 non-null  object\n",
      " 1   requirements  15185 non-null  object\n",
      "dtypes: object(2)\n",
      "memory usage: 279.5+ KB\n"
     ]
    }
   ],
   "source": [
    "data[['description', 'requirements']].info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.dropna(subset=['description', 'requirements'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['words'] = data['description'] + ' ' + data['requirements']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bring in stopwords\n",
    "sw = stopwords.words('english')\n",
    "# Regex pattern to locate 3 capitial letters back to back\n",
    "caps_pattern = r\"([A-Z]{3,})\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_wordnet_pos(treebank_tag):\n",
    "    '''\n",
    "    Translate nltk POS to wordnet tags\n",
    "    From phase 4 lecture 6\n",
    "    '''\n",
    "    if treebank_tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif treebank_tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif treebank_tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif treebank_tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return wordnet.NOUN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def doc_preparer(doc, stop_words=sw):\n",
    "    '''\n",
    "    Return: a document string with words which have been \n",
    "            lemmatized, \n",
    "            parsed for stopwords, \n",
    "            made lowercase,\n",
    "            and stripped of punctuation and numbers.\n",
    "    From phase 4 lecture 6\n",
    "    '''\n",
    "    \n",
    "    regex_token = RegexpTokenizer(r\"([a-zA-Z]+(?:’[a-z]+)?)\")\n",
    "    doc = regex_token.tokenize(doc)\n",
    "    doc = [word.lower() for word in doc]\n",
    "    doc = [word for word in doc if word not in sw]\n",
    "    # print(doc)\n",
    "    doc = pos_tag(doc)\n",
    "    doc = [(word[0], get_wordnet_pos(word[1])) for word in doc]\n",
    "    lemmatizer = WordNetLemmatizer() \n",
    "    doc = [lemmatizer.lemmatize(word[0], word[1]) for word in doc]\n",
    "    return ' '.join(doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To arrive at an intial approach, I'm going to analysis my data by first conducting a train test split for each model. Once I have split the data, I will use my `doc_preparer` function to tokenize the text. After the text is clean I will run a second train test split on the tokenized text so that we are able to build a model. Since this is a binary classification I will be using multinomial naive bayes to build models. I will also be using natural language processing since dealing with textual information. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baseline Understanding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    0.954689\n",
       "1    0.045311\n",
       "Name: fraudulent, dtype: float64"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Showings the probablities of fraudulent job openings\n",
    "y_train.value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Accuracy score of train data 0.9546891464699684\n",
      "************************************************\n",
      " Accuracy score of test data 0.9483803002370292\n"
     ]
    }
   ],
   "source": [
    "# Making predicts using baseline model\n",
    "preds_baseline = [0] * len(y_train)\n",
    "preds_test_baseline = [0] * len(y_test)\n",
    "# Showing us the accuracy score for modelless baseline\n",
    "print(f' Accuracy score of train data {accuracy_score(y_train, preds_baseline)}')\n",
    "print('*' *48)\n",
    "print(f' Accuracy score of test data {accuracy_score(y_test, preds_test_baseline)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A modelless baseline is just taking the majority of the two possibility. For my baseline model, the majority is job openings that are not fraudulent. Because it is majority, the easiest way to determine if a job opeing is fraudulent or not is to assume that it isn't. My binary catgories are very imbalanced, so i will be looking at accuracy as my metric for interperting my model. Based on the accuracy score of the train and test data, this baseline is slightly overfit. The score of the modeless based line is 95.5% but scored on unseen data it is 94.8%."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Description Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting our X and y variables\n",
    "X = data.description\n",
    "y = data.fraudulent\n",
    "# Settign up a train test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,\n",
    "                                                    y,\n",
    "                                                    random_state=42,\n",
    "                                                    test_size=0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using yhr doc_preparer function\n",
    "token_docs = [doc_preparer(doc, sw) for doc in X_train]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example text before the doc_preparer\n",
    "X_train.iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example text after the doc_preparer\n",
    "token_docs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Secondary train-test split to build our best model\n",
    "X_t, X_val, y_t, y_val = train_test_split(token_docs, y_train,\n",
    "                                          test_size=0.25, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Count Vectorizer & Multinomial Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate count vectorizer and fit on data\n",
    "cv = CountVectorizer()\n",
    "X_t_vec = cv.fit_transform(X_t)\n",
    "X_t_vec  = pd.DataFrame.sparse.from_spmatrix(X_t_vec)\n",
    "X_t_vec.columns = sorted(cv.vocabulary_)\n",
    "X_t_vec.set_index(y_t.index, inplace=True)\n",
    "\n",
    "# We then transform the validation set. (Do not refit the vectorizer!)\n",
    "X_val_vec = cv.transform(X_val)\n",
    "X_val_vec  = pd.DataFrame.sparse.from_spmatrix(X_val_vec)\n",
    "X_val_vec.columns = sorted(cv.vocabulary_)\n",
    "X_val_vec.set_index(y_val.index, inplace=True)\n",
    "\n",
    "# Now let's fit the the Multinomial Naive Bayes Classifier on our data and predict\n",
    "mnb = MultinomialNB()\n",
    "mnb.fit(X_t_vec, y_t)\n",
    "# Generate model predictions and plot confusion matrix\n",
    "y_hat = mnb.predict(X_val_vec)\n",
    "plot_confusion_matrix(mnb, X_val_vec, y_val);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(f'accuracy score {accuracy_score(y_val, y_hat)}')\n",
    "print(f'f1 score {f1_score(y_val, y_hat)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prior_0 = y_t.value_counts()[0]/len(y_t)\n",
    "prior_1 = y_t.value_counts()[1]/len(y_t)\n",
    "print(prior_0, prior_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(mnb.class_log_prior_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.log(prior_0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- This Multinomial naive bayes model using job descriptions has a accuracy score of 96.8%. This model is 1.8% better than the baseline model. There is a high true negative value like i wanted. The false positive and false negative values are not too bad. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Requirement Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting up our variables\n",
    "X = data.requirements\n",
    "y = data.fraudulent\n",
    "# Setting up for a train test split\n",
    "X_trainR, X_testR, y_trainR, y_testR = train_test_split(X,\n",
    "                                                    y,\n",
    "                                                    random_state=42,\n",
    "                                                    test_size=0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using my doc_preparer function\n",
    "token_docsR = [doc_preparer(doc, sw) for doc in X_trainR]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example text before doc_preparer\n",
    "X_trainR.iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example text after doc_preparer\n",
    "token_docsR[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Secondary train-test split to build our best model\n",
    "X_tR, X_valR, y_tR, y_valR = train_test_split(token_docsR, y_trainR,\n",
    "                                          test_size=0.25, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Count Vectorizer & Multinomial Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the count vectorizer and fitting the data\n",
    "cv = CountVectorizer()\n",
    "X_t_vecR = cv.fit_transform(X_tR)\n",
    "X_t_vecR  = pd.DataFrame.sparse.from_spmatrix(X_t_vecR)\n",
    "X_t_vecR.columns = sorted(cv.vocabulary_)\n",
    "X_t_vecR.set_index(y_tR.index, inplace=True)\n",
    "\n",
    "# We then transform the validation set. (Do not refit the vectorizer!)\n",
    "X_val_vecR = cv.transform(X_valR)\n",
    "X_val_vecR  = pd.DataFrame.sparse.from_spmatrix(X_val_vecR)\n",
    "X_val_vecR.columns = sorted(cv.vocabulary_)\n",
    "X_val_vecR.set_index(y_valR.index, inplace=True)\n",
    "\n",
    "# Now let's fit the the Multinomial Naive Bayes Classifier on our data and predict\n",
    "mnb = MultinomialNB()\n",
    "mnb.fit(X_t_vecR, y_tR)\n",
    "# Generate model predictions and plot confusion matrix\n",
    "y_hatR = mnb.predict(X_val_vecR)\n",
    "plot_confusion_matrix(mnb, X_val_vecR, y_valR);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'accuracy score {accuracy_score(y_valR, y_hatR)}')\n",
    "print(f'precision score {precision_score(y_valR, y_hatR)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prior_0 = y_tR.value_counts()[0]/len(y_tR)\n",
    "prior_1 = y_tR.value_counts()[1]/len(y_tR)\n",
    "print(prior_0, prior_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnb.class_log_prior_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.log(prior_0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- This Multinomial naive bayes model using job requirements has a accuracy score of 96.3%. This model is 1.3% better than the baseline model. The requirements model is slightly worst than the description model. There is a high true negative value like i wanted, higher than the description model too. The false positive and false negative values are not too bad."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Final Model**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Description and Requirements Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using the regex caps_pattern\n",
    "data['words'] = data['words'].str.replace(caps_pattern, r\" \").str.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting the variables\n",
    "X = data['words']\n",
    "y = data['fraudulent']\n",
    "# Setting up for a train test split\n",
    "X_trainDR, X_testDR, y_trainDR, y_testDR = train_test_split(X,\n",
    "                                                    y,\n",
    "                                                    random_state=42,\n",
    "                                                    test_size=0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using the doc_preparer function\n",
    "token_docsDR = [doc_preparer(doc, sw) for doc in X_trainDR]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example text before the doc_preparer\n",
    "X_trainDR[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example text after the doc_preparer\n",
    "token_docsDR[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Secondary train-test split to build our best model\n",
    "X_tDR, X_valDR, y_tDR, y_valDR = train_test_split(token_docsDR, y_train,\n",
    "                                          test_size=0.25, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Count Vectorizer & Multinomial Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the count vectorizer and fit on data\n",
    "cv = CountVectorizer()\n",
    "X_t_vecDR = cv.fit_transform(X_tDR)\n",
    "X_t_vecDR = pd.DataFrame.sparse.from_spmatrix(X_t_vecDR)\n",
    "X_t_vecDR.columns = sorted(cv.vocabulary_)\n",
    "X_t_vecDR.set_index(y_tDR.index, inplace=True)\n",
    "\n",
    "# We then transform the validation set. (Do not refit the vectorizer!)\n",
    "X_val_vecDR = cv.transform(X_valDR)\n",
    "X_val_vecDR  = pd.DataFrame.sparse.from_spmatrix(X_val_vecDR)\n",
    "X_val_vecDR.columns = sorted(cv.vocabulary_)\n",
    "X_val_vecDR.set_index(y_valDR.index, inplace=True)\n",
    "\n",
    "# Now let's fit the the Multinomial Naive Bayes Classifier on our data and predict\n",
    "mnb1 = MultinomialNB()\n",
    "mnb1.fit(X_t_vecDR, y_tDR)\n",
    "# Generate model predictions and plot confusion matrix\n",
    "y_hatDR = mnb1.predict(X_val_vecDR)\n",
    "plot_confusion_matrix(mnb1, X_val_vecDR, y_valDR);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'accuracy score {accuracy_score(y_valDR, y_hatDR)}')\n",
    "print(f'precision score {precision_score(y_valDR, y_hatDR)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prior_0 = y_tDR.value_counts()[0]/len(y_tDR)\n",
    "prior_1 = y_tDR.value_counts()[1]/len(y_tDR)\n",
    "print(prior_0, prior_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnb.class_log_prior_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.log(prior_0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- This Multinomial naive bayes model using both job description and requirements has a accuracy score of 97.2%. This model is 2% better than the baseline model. There is a high true negative value like i wanted, higher than both other models. The false positive and false negative values are not too bad."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving Final Model & Count Vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving final MNB model\n",
    "pickle.dump(mnb1, open(\"cv_model.sav\", 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving count vectorizer\n",
    "pickle.dump(cv, open(\"vectorizer.sav\", 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting a variable to the final model \n",
    "loaded_model = pickle.load(open(\"cv_model.sav\", 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Making sure it is predicting\n",
    "loaded_model.predict(X_val_vecDR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a pipeline for count vectorizer and model\n",
    "pipe = make_pipeline(cv, mnb1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe.predict_proba([token_docsDR[3]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiating the Lime text explainer\n",
    "explainer = LimeTextExplainer(class_names=['Not Fraudulent','Fraudulent'])\n",
    "exp = explainer.explain_instance(token_docsDR[3], \n",
    "                                 pipe.predict_proba, \n",
    "                                 num_features=20)\n",
    "\n",
    "print('Probability Fraudulent=', pipe.predict_proba([token_docsDR[3]])[0,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp.show_in_notebook(text=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp.as_pyplot_figure()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I would recommend that Indeed focus on looking more into both the description and requirements together for a higher chance of determining whether it is fraudulent or not. I would also recommend them to give their viewers direct access to my website that I created to make predictions based on both description and requirements. If a viewer is feeling suspicous about a job all they have to do it click a link that takes them directly to my website and paste in their text. A reason why my model has a possibility of not solving the problem everytime is because it is only 97% accurate which leaves 3% of the time being inaccurate. Future plans that I could improve is cleaning the text more, this process is never ending and adjustments can always be made. I would also like to increase true positives, which in this case is if the model predict an opening is fraulent and it is correct. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (learn-env)",
   "language": "python",
   "name": "learn-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
